{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤– Notebook 02: Building the RAG Pipeline with LangChain (FAISS)\n",
    "\n",
    "## Learning Objectives\n",
    "In this notebook, you will learn:\n",
    "1. **Load the vector store** from disk (no re-embedding needed)\n",
    "2. **Create a retriever** for finding relevant documents\n",
    "3. **Load an LLM** (FLAN-T5-small) for text generation\n",
    "4. **Build a RAG prompt** that instructs the LLM to use context\n",
    "5. **Run the complete RAG pipeline** and see answers with sources\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "**RAG = Retrieval-Augmented Generation**\n",
    "\n",
    "It's a technique that combines:\n",
    "1. **Retrieval**: Find relevant documents from a knowledge base\n",
    "2. **Augmentation**: Add those documents as context to a prompt\n",
    "3. **Generation**: Use an LLM to generate an answer based on the context\n",
    "\n",
    "Why RAG?\n",
    "- LLMs can hallucinate (make up facts)\n",
    "- RAG grounds the LLM in your actual data (support tickets)\n",
    "- You can update the knowledge base without retraining\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# IMPORTANT: Set up HuggingFace cache BEFORE importing transformers\n",
    "from src.config import setup_hf_cache\n",
    "setup_hf_cache()\n",
    "\n",
    "print(\"âœ“ Setup complete!\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our custom modules\n",
    "from src import config\n",
    "from src.vectorstore import load_vector_store, get_retriever\n",
    "from src.llm import get_llm, test_llm, RAG_PROMPT_TEMPLATE, format_docs_for_context\n",
    "from src.rag_pipeline import RAGPipeline, print_rag_response\n",
    "\n",
    "print(\"âœ“ Custom modules imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Load the Vector Store\n",
    "\n",
    "We'll load the **FAISS** vector store we created in Notebook 01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vector store from disk\n",
    "vectorstore = load_vector_store()\n",
    "\n",
    "print(f\"\\nâœ“ Vector store loaded from: {config.VECTOR_STORE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever\n",
    "# k=5 means we'll retrieve the top 5 most similar documents\n",
    "retriever = get_retriever(vectorstore, k=5)\n",
    "\n",
    "print(\"\\nâœ“ Retriever created (k=5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Load the LLM\n",
    "\n",
    "We'll use **google/flan-t5-small** - a small, CPU-friendly model.\n",
    "\n",
    "### About FLAN-T5-small:\n",
    "- ~300MB download (first time only)\n",
    "- Instruction-tuned (good at following prompts)\n",
    "- Runs on CPU (no GPU needed)\n",
    "- Good for demos and learning\n",
    "\n",
    "**Note:** First run will download the model. This is cached for future runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show LLM configuration\n",
    "print(\"LLM Configuration:\")\n",
    "print(f\"  Model: {config.LLM_MODEL_NAME}\")\n",
    "print(f\"  Cache directory: {config.MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LLM\n",
    "# This will download the model on first run (~300MB)\n",
    "llm = get_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test to make sure the LLM is working\n",
    "test_response = test_llm(llm)\n",
    "print(\"\\nâœ“ LLM is working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Understand the RAG Prompt\n",
    "\n",
    "The prompt is **critical** for RAG. It tells the LLM:\n",
    "1. What role to play (support analytics assistant)\n",
    "2. To use ONLY the provided context\n",
    "3. To say \"I don't know\" if context is insufficient\n",
    "\n",
    "This helps prevent hallucination!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at our RAG prompt template\n",
    "print(\"RAG PROMPT TEMPLATE\")\n",
    "print(\"=\" * 60)\n",
    "print(RAG_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key elements of the prompt:**\n",
    "- **Role**: \"helpful customer support analytics assistant\"\n",
    "- **Instruction**: Use ONLY the provided context\n",
    "- **Fallback**: Say \"I don't have enough information\" if needed\n",
    "- **Placeholders**: `{context}` and `{question}` will be filled in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Build the RAG Pipeline\n",
    "\n",
    "Now let's put it all together! The `RAGPipeline` class handles:\n",
    "1. Retrieving relevant documents\n",
    "2. Formatting them into context\n",
    "3. Generating an answer with the LLM\n",
    "4. Returning the answer + sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RAG pipeline\n",
    "# We pass in our pre-loaded vectorstore and llm to avoid reloading\n",
    "rag = RAGPipeline(vectorstore=vectorstore, llm=llm, retrieval_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Ask Questions!\n",
    "\n",
    "Let's test the RAG pipeline with various questions about support tickets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Common billing issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask about billing issues\n",
    "question1 = \"What are the most common billing and payment issues customers report?\"\n",
    "\n",
    "response1 = rag.ask(question1)\n",
    "print_rag_response(response1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Technical problems with TVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask about TV issues\n",
    "question2 = \"What technical problems do customers face with smart TVs?\"\n",
    "\n",
    "response2 = rag.ask(question2)\n",
    "print_rag_response(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Refund requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask about refunds\n",
    "question3 = \"How are refund requests typically handled? What are common reasons for refunds?\"\n",
    "\n",
    "response3 = rag.ask(question3)\n",
    "print_rag_response(response3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Critical priority tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask about critical issues\n",
    "question4 = \"What types of issues are marked as critical priority?\"\n",
    "\n",
    "response4 = rag.ask(question4)\n",
    "print_rag_response(response4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Device connectivity issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask about connectivity\n",
    "question5 = \"What connectivity or network issues do customers report with their devices?\"\n",
    "\n",
    "response5 = rag.ask(question5)\n",
    "print_rag_response(response5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Examine the RAG Process in Detail\n",
    "\n",
    "Let's break down what happens at each step of the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's trace through a question step by step\n",
    "demo_question = \"What problems do customers have with laptop batteries?\"\n",
    "\n",
    "print(\"STEP-BY-STEP RAG PROCESS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n1. QUESTION: {demo_question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Retrieve relevant documents\n",
    "print(\"\\n2. RETRIEVAL: Finding similar documents...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "retrieved_docs = rag.retrieve(demo_question)\n",
    "\n",
    "print(f\"   Retrieved {len(retrieved_docs)} documents\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"\\n   Doc {i}: Ticket {doc.metadata.get('ticket_id', 'N/A')}\")\n",
    "    print(f\"   Product: {doc.metadata.get('product', 'N/A')}\")\n",
    "    print(f\"   Preview: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Format context\n",
    "print(\"\\n3. CONTEXT FORMATTING:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "context = format_docs_for_context(retrieved_docs)\n",
    "print(f\"   Context length: {len(context)} characters\")\n",
    "print(f\"\\n   Context preview (first 500 chars):\")\n",
    "print(f\"   {context[:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate answer\n",
    "print(\"\\n4. GENERATION: Calling LLM...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "answer = rag.generate(demo_question, context)\n",
    "print(f\"\\n   Generated Answer:\")\n",
    "print(f\"   {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Get Detailed Response Information\n",
    "\n",
    "For debugging and evaluation, we can get more details about each response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed response as dictionary\n",
    "detailed = rag.ask_with_details(\"What issues do customers have with GoPro cameras?\")\n",
    "\n",
    "print(\"DETAILED RESPONSE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nQuestion: {detailed['question']}\")\n",
    "print(f\"\\nAnswer: {detailed['answer']}\")\n",
    "print(f\"\\nNumber of sources: {detailed['num_sources']}\")\n",
    "print(f\"Context length: {detailed['context_length']} chars\")\n",
    "print(\"\\nSources:\")\n",
    "for i, src in enumerate(detailed['sources'], 1):\n",
    "    print(f\"  {i}. Ticket {src['ticket_id']} - {src['product']} ({src['ticket_type']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What We Accomplished\n",
    "1. âœ… Loaded the vector store from disk\n",
    "2. âœ… Created a retriever (k=5)\n",
    "3. âœ… Loaded FLAN-T5-small LLM\n",
    "4. âœ… Built the complete RAG pipeline\n",
    "5. âœ… Asked 5+ questions and got answers with sources\n",
    "6. âœ… Traced through the RAG process step-by-step\n",
    "\n",
    "### Key Takeaways\n",
    "- **RAG = Retrieval + Augmentation + Generation**\n",
    "- The **prompt** is critical - it tells the LLM to use context\n",
    "- **Sources** provide transparency and traceability\n",
    "- Small models like FLAN-T5 work for demos but have limitations\n",
    "\n",
    "### Limitations of FLAN-T5-small\n",
    "- Answers may be short or incomplete\n",
    "- May not fully utilize all context\n",
    "- For production, consider larger models or API-based LLMs\n",
    "\n",
    "### Next Steps\n",
    "â†’ **Notebook 03**: Evaluate RAG quality systematically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸŽ‰ Notebook 02 Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nYou've built a working RAG pipeline!\")\n",
    "print(\"\\nProceed to: 03_rag_evaluation.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
