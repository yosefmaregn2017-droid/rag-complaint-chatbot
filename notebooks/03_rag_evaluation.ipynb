{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Notebook 03: RAG Evaluation\n",
    "\n",
    "## Learning Objectives\n",
    "In this notebook, you will learn:\n",
    "1. **Why evaluate RAG systems** - understanding failure modes\n",
    "2. **Create evaluation questions** covering different aspects\n",
    "3. **Run systematic evaluation** and collect results\n",
    "4. **Score responses manually** using a clear rubric\n",
    "5. **Generate evaluation reports** in markdown format\n",
    "6. **Debug and tune** retrieval parameters (k, chunk size)\n",
    "\n",
    "## Why Evaluate RAG?\n",
    "\n",
    "RAG systems can fail in many ways:\n",
    "- **Retrieval failures**: Wrong documents retrieved\n",
    "- **Context ignored**: LLM doesn't use the provided context\n",
    "- **Hallucination**: LLM makes up information not in context\n",
    "- **Incomplete answers**: Important details missing\n",
    "\n",
    "Systematic evaluation helps identify and fix these issues.\n",
    "\n",
    "## Evaluation Dimensions\n",
    "\n",
    "| Dimension | Question to Ask |\n",
    "|-----------|----------------|\n",
    "| **Retrieval Quality** | Are the right documents being retrieved? |\n",
    "| **Answer Accuracy** | Is the answer factually correct? |\n",
    "| **Faithfulness** | Does the answer stick to the context? |\n",
    "| **Relevance** | Does the answer address the question? |\n",
    "| **Completeness** | Are all important points covered? |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Set up HuggingFace cache\n",
    "from src.config import setup_hf_cache\n",
    "setup_hf_cache()\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "print(\"‚úì Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our custom modules\n",
    "from src import config\n",
    "from src.rag_pipeline import RAGPipeline, print_rag_response\n",
    "from src.evaluation import (\n",
    "    SAMPLE_EVALUATION_QUESTIONS,\n",
    "    run_evaluation,\n",
    "    results_to_dataframe,\n",
    "    results_to_markdown_table,\n",
    "    print_scoring_guide,\n",
    "    EvaluationResult,\n",
    "    EvaluationReport,\n",
    "    compare_retrieval_k,\n",
    ")\n",
    "from src.vectorstore import load_vector_store, search_similar\n",
    "\n",
    "print(\"‚úì Custom modules imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Initialize the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG pipeline\n",
    "# This loads the vector store and LLM\n",
    "rag = RAGPipeline(retrieval_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Review the Scoring Guide\n",
    "\n",
    "Before evaluating, let's understand how to score responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the scoring guide\n",
    "print_scoring_guide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Define Evaluation Questions\n",
    "\n",
    "We'll create questions that cover different aspects of the support ticket data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our evaluation questions\n",
    "# These cover different categories to test RAG comprehensively\n",
    "\n",
    "evaluation_questions = [\n",
    "    # Product-specific questions\n",
    "    \"What are the most common issues reported for smart TVs?\",\n",
    "    \"What problems do customers face with GoPro cameras?\",\n",
    "    \n",
    "    # Issue type questions  \n",
    "    \"What are typical billing and payment issues customers report?\",\n",
    "    \"What technical issues are most frequently mentioned?\",\n",
    "    \n",
    "    # Priority/severity questions\n",
    "    \"What types of issues are marked as critical priority?\",\n",
    "    \"What patterns do you see in high priority tickets?\",\n",
    "    \n",
    "    # Resolution questions\n",
    "    \"How are refund requests typically handled?\",\n",
    "    \"What solutions are provided for device connectivity issues?\",\n",
    "    \n",
    "    # Channel-specific questions\n",
    "    \"What issues come through social media channels?\",\n",
    "    \"Are there differences in issues reported via email vs chat?\",\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(evaluation_questions)} evaluation questions:\")\n",
    "print(\"-\" * 50)\n",
    "for i, q in enumerate(evaluation_questions, 1):\n",
    "    print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Run Evaluation\n",
    "\n",
    "Let's run all questions through the RAG pipeline and collect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on all questions\n",
    "report = run_evaluation(rag, evaluation_questions, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Review Results and Score\n",
    "\n",
    "Now let's review each result in detail and assign scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review each result\n",
    "print(\"DETAILED EVALUATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, result in enumerate(report.results, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"QUESTION {i}: {result.question}\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nANSWER:\\n{result.answer}\")\n",
    "    print(f\"\\nSOURCES: {result.sources_summary}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Scoring\n",
    "\n",
    "Based on the results above, let's assign scores. \n",
    "\n",
    "**Instructions:**\n",
    "1. Review each answer and its sources\n",
    "2. Assign a score 1-5 based on the scoring guide\n",
    "3. Add comments explaining your score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the results\n",
    "# Modify these scores based on your review of the outputs above!\n",
    "\n",
    "# Example scoring (you should adjust based on actual outputs)\n",
    "scores_and_comments = [\n",
    "    (3, \"Provides some relevant info about TV issues, but could be more specific\"),\n",
    "    (3, \"Mentions GoPro issues but answer is brief\"),\n",
    "    (4, \"Good coverage of billing issues from retrieved tickets\"),\n",
    "    (3, \"Lists some technical issues, sources are relevant\"),\n",
    "    (3, \"Identifies critical issues but limited detail\"),\n",
    "    (3, \"Some patterns identified, could use more analysis\"),\n",
    "    (4, \"Good explanation of refund handling process\"),\n",
    "    (3, \"Mentions connectivity solutions, sources relevant\"),\n",
    "    (3, \"Some social media issues identified\"),\n",
    "    (2, \"Limited differentiation between channels\"),\n",
    "]\n",
    "\n",
    "# Apply scores to results\n",
    "for i, (score, comment) in enumerate(scores_and_comments):\n",
    "    if i < len(report.results):\n",
    "        report.results[i].score = score\n",
    "        report.results[i].comments = comment\n",
    "\n",
    "print(\"‚úì Scores applied to results\")\n",
    "print(f\"\\nAverage score: {report.average_score:.2f} / 5.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Generate Evaluation Report\n",
    "\n",
    "Let's create a formatted evaluation report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for easy viewing\n",
    "eval_df = results_to_dataframe(report)\n",
    "\n",
    "print(\"EVALUATION RESULTS TABLE\")\n",
    "print(\"=\" * 70)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate markdown table\n",
    "markdown_report = results_to_markdown_table(report)\n",
    "\n",
    "print(\"MARKDOWN EVALUATION TABLE\")\n",
    "print(\"=\" * 70)\n",
    "print(markdown_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the markdown report to a file\n",
    "report_path = project_root / \"evaluation_report.md\"\n",
    "\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(\"# RAG Evaluation Report\\n\\n\")\n",
    "    f.write(f\"**Date:** {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}\\n\\n\")\n",
    "    f.write(f\"**Configuration:**\\n\")\n",
    "    f.write(f\"- Retrieval k: {rag.retrieval_k}\\n\")\n",
    "    f.write(f\"- Chunk size: {config.CHUNK_SIZE}\\n\")\n",
    "    f.write(f\"- Chunk overlap: {config.CHUNK_OVERLAP}\\n\")\n",
    "    f.write(f\"- LLM: {config.LLM_MODEL_NAME}\\n\\n\")\n",
    "    f.write(\"## Results\\n\\n\")\n",
    "    f.write(markdown_report)\n",
    "\n",
    "print(f\"‚úì Report saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Debug Section - Parameter Tuning\n",
    "\n",
    "Let's explore how different parameters affect RAG performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Varying Retrieval k\n",
    "\n",
    "How does the number of retrieved documents affect results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare retrieval with different k values\n",
    "test_question = \"What are common billing issues?\"\n",
    "\n",
    "print(\"COMPARING RETRIEVAL WITH DIFFERENT k VALUES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Question: {test_question}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for k in [3, 5, 8]:\n",
    "    print(f\"\\n--- k = {k} ---\")\n",
    "    results = search_similar(rag.vectorstore, test_question, k=k)\n",
    "    print(f\"Retrieved {len(results)} documents:\")\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"  {i}. Ticket {doc.metadata.get('ticket_id', 'N/A')} - {doc.metadata.get('product', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare answers with different k values\n",
    "print(\"\\nCOMPARING ANSWERS WITH DIFFERENT k VALUES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for k in [3, 8]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"k = {k}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create temporary pipeline with different k\n",
    "    temp_rag = RAGPipeline(\n",
    "        vectorstore=rag.vectorstore, \n",
    "        llm=rag.llm, \n",
    "        retrieval_k=k\n",
    "    )\n",
    "    \n",
    "    response = temp_rag.ask(test_question)\n",
    "    print(f\"\\nAnswer: {response.answer}\")\n",
    "    print(f\"\\nSources: {len(response.sources)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Analysis: k=3 vs k=5 vs k=8\n",
    "\n",
    "| k Value | Pros | Cons |\n",
    "|---------|------|------|\n",
    "| k=3 | Faster, more focused | May miss relevant info |\n",
    "| k=5 | Good balance | Default choice |\n",
    "| k=8 | More context | May include noise, slower |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Chunk Size Impact\n",
    "\n",
    "Different chunk sizes affect retrieval precision:\n",
    "\n",
    "| Chunk Size | Pros | Cons |\n",
    "|------------|------|------|\n",
    "| 300 chars | More precise matching | Less context per chunk |\n",
    "| 500 chars | Balanced (our default) | Good for most cases |\n",
    "| 800 chars | More context | Less precise, may miss specific info |\n",
    "\n",
    "**Note:** Changing chunk size requires rebuilding the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show current chunk configuration\n",
    "print(\"CURRENT CHUNK CONFIGURATION\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Chunk size: {config.CHUNK_SIZE} characters\")\n",
    "print(f\"Chunk overlap: {config.CHUNK_OVERLAP} characters\")\n",
    "print(f\"\\nTo test different chunk sizes:\")\n",
    "print(\"1. Modify config.CHUNK_SIZE in src/config.py\")\n",
    "print(\"2. Re-run Notebook 01 to rebuild the vector store\")\n",
    "print(\"3. Re-run evaluation to compare results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 9: Recommendations Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations based on evaluation\n",
    "print(\"EVALUATION SUMMARY & RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "avg_score = report.average_score\n",
    "print(f\"\\nüìä Average Score: {avg_score:.2f} / 5.0\")\n",
    "\n",
    "if avg_score >= 4.0:\n",
    "    print(\"\\n‚úÖ RAG pipeline is performing well!\")\n",
    "elif avg_score >= 3.0:\n",
    "    print(\"\\n‚ö†Ô∏è RAG pipeline is acceptable but has room for improvement.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå RAG pipeline needs significant improvement.\")\n",
    "\n",
    "print(\"\\nüìã RECOMMENDATIONS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "recommendations = [\n",
    "    \"1. Try a larger LLM (flan-t5-base or flan-t5-large) for better answers\",\n",
    "    \"2. Experiment with k=3 to k=8 to find optimal retrieval count\",\n",
    "    \"3. Consider chunk_size=300 for more precise retrieval\",\n",
    "    \"4. Add more specific prompting for structured answers\",\n",
    "    \"5. For production: use API-based LLMs (OpenAI, Anthropic) for quality\",\n",
    "]\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(f\"  {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What We Accomplished\n",
    "1. ‚úÖ Defined 10 evaluation questions covering different aspects\n",
    "2. ‚úÖ Ran systematic evaluation on the RAG pipeline\n",
    "3. ‚úÖ Scored responses using a clear rubric (1-5 scale)\n",
    "4. ‚úÖ Generated evaluation reports (DataFrame and Markdown)\n",
    "5. ‚úÖ Explored parameter tuning (k values, chunk sizes)\n",
    "6. ‚úÖ Documented recommendations for improvement\n",
    "\n",
    "### Key Takeaways\n",
    "- **Systematic evaluation** is essential for RAG quality\n",
    "- **Manual scoring** provides ground truth (but is time-consuming)\n",
    "- **Parameter tuning** (k, chunk size) significantly impacts results\n",
    "- **Small LLMs** have limitations - consider larger models for production\n",
    "\n",
    "### Files Created\n",
    "- `evaluation_report.md` - Markdown evaluation report\n",
    "\n",
    "### Next Steps\n",
    "- Try the Streamlit app (`app.py`) for interactive Q&A\n",
    "- Experiment with different parameters\n",
    "- Consider using larger LLMs for better quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ Notebook 03 Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nYou've completed the RAG evaluation!\")\n",
    "print(f\"\\nFinal Average Score: {report.average_score:.2f} / 5.0\")\n",
    "print(f\"\\nReport saved to: {report_path}\")\n",
    "print(\"\\nüöÄ Try the Streamlit app: streamlit run app.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
